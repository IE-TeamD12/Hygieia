{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 for FIT5212, Semester 1, 2020\n",
    "\n",
    "**Student Name:**  Meghna Khanna\n",
    "\n",
    "**Student ID:**    29592445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Text Classification\n",
    "\n",
    "General comments and any shared processing here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Neural Network Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/train-set/axcs_train.csv\n",
      "/kaggle/input/test-set/axcs_test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "## Fast ai is a library that is mainly used for pretrained language models and fine tuning those models.\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *  ## import this libarary to access functions of the text.data class to perform NLP tasks\n",
    "from fastai.core import *\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>InfoTheory</th>\n",
       "      <th>CompVis</th>\n",
       "      <th>Math</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs-9301111</td>\n",
       "      <td>arxiv.org/abs/cs/9301111</td>\n",
       "      <td>1989-12-31</td>\n",
       "      <td>Nested satisfiability</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nested satisfiability A special case of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cs-9301112</td>\n",
       "      <td>arxiv.org/abs/cs/9301112</td>\n",
       "      <td>1990-03-31</td>\n",
       "      <td>A note on digitized angles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A note on digitized angles We study the confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs-9301113</td>\n",
       "      <td>arxiv.org/abs/cs/9301113</td>\n",
       "      <td>1991-07-31</td>\n",
       "      <td>Textbook examples of recursion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Textbook examples of recursion We discuss pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cs-9301114</td>\n",
       "      <td>arxiv.org/abs/cs/9301114</td>\n",
       "      <td>1991-10-31</td>\n",
       "      <td>Theory and practice</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Theory and practice The author argues to Sili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cs-9301115</td>\n",
       "      <td>arxiv.org/abs/cs/9301115</td>\n",
       "      <td>1991-11-30</td>\n",
       "      <td>Context-free multilanguages</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Context-free multilanguages This article is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                       URL        Date  \\\n",
       "0  cs-9301111  arxiv.org/abs/cs/9301111  1989-12-31   \n",
       "1  cs-9301112  arxiv.org/abs/cs/9301112  1990-03-31   \n",
       "2  cs-9301113  arxiv.org/abs/cs/9301113  1991-07-31   \n",
       "3  cs-9301114  arxiv.org/abs/cs/9301114  1991-10-31   \n",
       "4  cs-9301115  arxiv.org/abs/cs/9301115  1991-11-30   \n",
       "\n",
       "                            Title  InfoTheory  CompVis  Math  \\\n",
       "0           Nested satisfiability           0        0     0   \n",
       "1      A note on digitized angles           0        0     0   \n",
       "2  Textbook examples of recursion           0        0     0   \n",
       "3             Theory and practice           0        0     0   \n",
       "4     Context-free multilanguages           0        0     0   \n",
       "\n",
       "                                            Abstract  \n",
       "0   Nested satisfiability A special case of the s...  \n",
       "1   A note on digitized angles We study the confi...  \n",
       "2   Textbook examples of recursion We discuss pro...  \n",
       "3   Theory and practice The author argues to Sili...  \n",
       "4   Context-free multilanguages This article is a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading the training datasets using the pandas library function read_csv to load the dataset on to the working environment and using head() to have a glimpse of what the data is. \n",
    "\n",
    "train_set = pd.read_csv('/kaggle/input/train-set/axcs_train.csv')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>InfoTheory</th>\n",
       "      <th>CompVis</th>\n",
       "      <th>Math</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no-150100335</td>\n",
       "      <td>arxiv.org/abs/1501.00335</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>A Data Transparency Framework for Mobile Appli...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A Data Transparency Framework for Mobile Appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no-14024178</td>\n",
       "      <td>arxiv.org/abs/1402.4178</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>A reclaimer scheduling problem arising in coal...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A reclaimer scheduling problem arising in coa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no-150100263</td>\n",
       "      <td>arxiv.org/abs/1501.00263</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Communication-Efficient Distributed Optimizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Communication-Efficient Distributed Optimizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no-150100287</td>\n",
       "      <td>arxiv.org/abs/1501.00287</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Consistent Classification Algorithms for Multi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consistent Classification Algorithms for Mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no-11070586</td>\n",
       "      <td>arxiv.org/abs/1107.0586</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Managing key multicasting through orthogonal s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Managing key multicasting through orthogonal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                       URL        Date  \\\n",
       "0  no-150100335  arxiv.org/abs/1501.00335  2015-01-01   \n",
       "1   no-14024178   arxiv.org/abs/1402.4178  2015-01-01   \n",
       "2  no-150100263  arxiv.org/abs/1501.00263  2015-01-01   \n",
       "3  no-150100287  arxiv.org/abs/1501.00287  2015-01-01   \n",
       "4   no-11070586   arxiv.org/abs/1107.0586  2015-01-01   \n",
       "\n",
       "                                               Title  InfoTheory  CompVis  \\\n",
       "0  A Data Transparency Framework for Mobile Appli...         0.0      0.0   \n",
       "1  A reclaimer scheduling problem arising in coal...         0.0      0.0   \n",
       "2  Communication-Efficient Distributed Optimizati...         0.0      0.0   \n",
       "3  Consistent Classification Algorithms for Multi...         0.0      0.0   \n",
       "4  Managing key multicasting through orthogonal s...         0.0      0.0   \n",
       "\n",
       "   Math                                           Abstract  \n",
       "0   0.0   A Data Transparency Framework for Mobile Appl...  \n",
       "1   0.0   A reclaimer scheduling problem arising in coa...  \n",
       "2   1.0   Communication-Efficient Distributed Optimizat...  \n",
       "3   0.0   Consistent Classification Algorithms for Mult...  \n",
       "4   0.0   Managing key multicasting through orthogonal ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading the test data using the same pandas functions and head() this dataset will be used to check the accuracy of our classifier on unknown data, data that the classifier has not seen before\n",
    "\n",
    "test_set = pd.read_csv('/kaggle/input/test-set/axcs_test.csv')\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier - 1\n",
    "\n",
    "### Building a neural network classfier to classify Math when provided with the Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nested satisfiability A special case of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A note on digitized angles We study the confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Textbook examples of recursion We discuss pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Theory and practice The author argues to Sili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Context-free multilanguages This article is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math                                           Abstract\n",
       "0     0   Nested satisfiability A special case of the s...\n",
       "1     0   A note on digitized angles We study the confi...\n",
       "2     0   Textbook examples of recursion We discuss pro...\n",
       "3     0   Theory and practice The author argues to Sili...\n",
       "4     0   Context-free multilanguages This article is a..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Separating the columns Math and Abstract to train the classifier to learn features to predict the class as Maths if given an abstract or some text. \n",
    "\n",
    "train_Math= train_set[['Math','Abstract']]\n",
    "train_Math.head()  ## Glimpse of the modified sliced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54731, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Math.shape  ## Validate the shape of the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19678, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform the same operations on the test dataset to prepare it for processing in the language model and remove the NA values in the test set as there is only one NA value and it should \n",
    "## be removed and validate the shape of the dataset \n",
    "\n",
    "test_Math = test_set[['Math', 'Abstract']]  \n",
    "test_Math= test_Math.dropna(how='all')\n",
    "test_Math.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([train_Math, test_Math])  ## Merge the abstracts from test and train set to form a merged dataframe for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Nested satisfiability A special case of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>A note on digitized angles We study the confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Textbook examples of recursion We discuss pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Theory and practice The author argues to Sili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Context-free multilanguages This article is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math                                           Abstract\n",
       "0   0.0   Nested satisfiability A special case of the s...\n",
       "1   0.0   A note on digitized angles We study the confi...\n",
       "2   0.0   Textbook examples of recursion We discuss pro...\n",
       "3   0.0   Theory and practice The author argues to Sili...\n",
       "4   0.0   Context-free multilanguages This article is a..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()  ## Check the view of the new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickly assembling our data to make use of the fast ai library to build a language model in order to perform text classification. The library we use is the text.data library which is made available using the data_block api. We use the function called *TextLMDataBunch* that is used to get the data into a specfied model to parse to the language model. The parameters are the *path* which is a writable path in which the language model will be stored and then the training dataset  and validation dataset is merged_df that has all the text provided and the *device* is cuda to access the GPU so that the operations are faster and are done simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Storing the data structure in a variable called databunch.\n",
    "\n",
    "data_bunch = TextLMDataBunch.from_df(path='/kaggle/working',train_df= merged_df,valid_df=merged_df, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glimpse of the databunch just created, and it is displayed using the show_batch function. The sentences have been tokenized and as we can see tags have added to the words all special and grammatical characters have been retained. Since it is easier for the machine to interpret numbers the tokens that hav been generated have been replaced with the location of the token in the vocabulary. The voacbulary has been created when we create the databunch it encodes the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>of recursion xxmaj we discuss properties of recursive schemas related to mccarthy 's ` ` 91 function ' ' and to xxmaj takeuchi 's triple recursion . xxmaj several theorems are proposed as interesting candidates for machine verification , and some intriguing open questions are raised . xxbos xxmaj theory and practice xxmaj the author argues to xxmaj silicon xxmaj valley that the most important and powerful part of computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>models and its application to belief revision and truth maintenance xxmaj we introduced decomposable negation normal form xxup dnnf recently as a tractable form of propositional theories , and provided a number of powerful logical operations that can be performed on it in polynomial time . xxmaj we also presented an algorithm for compiling any conjunctive normal form xxup cnf into xxup dnnf and provided a structure - based guarantee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>soon as the program under consideration exploits more than a few processors over a long execution time . xxmaj this problem is addressed by the novel debugging tool dewiz ( xxmaj debugging xxmaj wizard ) , whose focus lies on scalability . dewiz has a modular , scalable architecture , and uses the event graph model as a representation of the investigated program . dewiz provides a set of modules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>xxmaj toward xxmaj compact xxmaj interdomain xxmaj routing xxmaj this paper critically examines some propositions and arguments of cs . xxup ni / xxunk regarding applicability of hierarchical routing and perspectives of compact routing . xxmaj arguments against the former are found to be inaccurate while the latter is found to be equivalent to well - known deployed solutions . xxmaj also , multiple ( stacked ) application of compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>algebraic xxmaj parametric xxmaj systems by xxmaj rectification of their xxmaj affine xxmaj expanded xxmaj lie xxmaj symmetries xxmaj lie group theory states that knowledge of a parameters solvable group of symmetries of a system of ordinary differential equations allows to reduce by the number of equations . xxmaj we apply this principle by finding some \\ xxunk derivations } that induces \\ xxunk } xxmaj lie point symmetries of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Glimpse of the databunch that was just created using the show_batch function. \n",
    "\n",
    "data_bunch.show_batch()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are using the pre-trained model `AWD_LSTM` which has been trained on the `wikitext` and has recorded the weights. The objective of creating a language learner is for contextualized learning and the lower representations of words in the text to give the text semantic meaning. This model is trained to predict the next word given a sequence of words and it is traiend in an unsupervised manner using `Recurrent Neural Networks(RNN)` to train the language model learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The learner has the parameters the data that is the databunch and name of the pre-trained model that will be downloaded from fastai and the directory where the learner model will be stored and drop_multi is a regularization parameter so we increase the penalization that is increase the paarmeter when the model is overfitting and decrease it when the model is underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner = language_model_learner(data_bunch, AWD_LSTM, drop_mult=0.7, model_dir='/kaggle/working/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we fit the learner model for 1 epoch and a learning rate of 1e-3 as the starting learning rate. We check the accuarcy, and loss in one epoch and as we see the accuracy is 32%. Here the learner is only training the last few layers of the RNN and the rest of the model and layers are left as it is, the last layers learn the weights of the new language and the reamining layers are the weights are the pre-trained model weights. We are training the learner to learn the new language that it has been presented with. The function fit_one_cycle is provided by fast_ai. We first just run for one epoch and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.302774</td>\n",
       "      <td>4.058744</td>\n",
       "      <td>0.320315</td>\n",
       "      <td>16:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step is to unfreeze the all the layers and train all the layers, but before that we check if the learning rate we ahd used was of optimum value or not so we use a fastai function called lr_find() which finds the best learning rate value for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='3174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      3.12% [99/3174 00:23<12:25 5.8693]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.unfreeze()\n",
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After plotting the loss at it each learning rate we can see that the learning rate skyrockets after 1e-3 the loss starts increasing if we set the learning rate to anything greater than the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">  Now we fit the model to all the layers the pretrained layers and the new layers,the weights will be optimised as we are training on all the layers of the language learner and the accuracy increases by about 10% and the loss decreases with every epoch. We run fit_one_cycle() which consists of a callback to the OneCycleScheduler which uses the 1cycle policy. The 1cycle policy for Cyclical Learning Rates is that the learning rate exists within a reasonable mininum and maximum bounds basically the learning rate increases for one step and decreases for the other(one step consists of some iterations) so that incraeses the convergence and training time. This is used as low learning rate is too slow but it is accurate and a big learning rate crosses the sweet spot and diverges so to overcome this problem we use CLR. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.908977</td>\n",
       "      <td>3.712025</td>\n",
       "      <td>0.352563</td>\n",
       "      <td>16:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.752786</td>\n",
       "      <td>3.509501</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>16:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.638067</td>\n",
       "      <td>3.403575</td>\n",
       "      <td>0.382405</td>\n",
       "      <td>16:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.580252</td>\n",
       "      <td>3.374038</td>\n",
       "      <td>0.385717</td>\n",
       "      <td>16:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## the first parameter is epcoh, then learning rate value and last is the momentum parameter. The moms parameter is for the reason that when the learning rate is slow and we are going in \n",
    "## the same direction we might as well go faster and when the learning rate is high the momentum is slow as if we go fats we might overshoot the target that is why that is a parameter that\n",
    "## should be specified.\n",
    "\n",
    "learner.fit_one_cycle(4,1e-3,moms=(0.8,0.7))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we save the encoder to a writable folder and named it encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('/kaggle/working/encoder')  ## Inbuilt function used save_encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we create a text data bunch only of the train set as here we are training the model to predict the class Math and we should not use the test set as we would be classifying ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The vocabulary learnt from the language data bunch is added as teh vocab parameter and bs is the batch size is 32 which is the size of the batch.\n",
    "## Batch size bs to be used according to the GPU memory we have available, so for a 8GB GPU around bs=32 works fine. \n",
    "\n",
    "\n",
    "data_clas = TextClasDataBunch.from_df('./', train_df=train_Math, valid_df=train_Math, test_df= test_Math, vocab=data_bunch.train_ds.vocab, bs=32,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj some xxmaj extensions of xxmaj probabilistic xxmaj logic xxmaj in [ 12 ] , xxmaj nilsson proposed the probabilistic logic in which the truth values of logical propositions are probability values between 0 and 1 . xxmaj it is applicable to any logical system for which the consistency of a finite set of propositions can be established . xxmaj the probabilistic inference scheme reduces to the ordinary logical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj magnetic xxmaj towers of xxmaj hanoi and their xxmaj optimal xxmaj solutions xxmaj the xxmaj magnetic xxmaj tower of xxmaj hanoi puzzle - a modified base 3 version of the classical xxmaj tower of xxmaj hanoi puzzle as described in earlier papers , is actually a small set of independent sister - puzzles , depending on the pre - coloring combination of the tower 's posts . xxmaj</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj optimal xxmaj planar xxmaj range xxmaj skyline xxmaj reporting with xxmaj linear xxmaj space in xxmaj external xxmaj memory xxmaj let p be a set of n points in xxup r^2 . xxmaj given a rectangle q = [ \\ alpha_1 , \\ alpha_2 ] x [ \\ beta_1 , \\ beta_2 ] , a range skyline query returns the maxima of the points in p \\ cap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj full - xxmaj rate , xxmaj full - xxmaj diversity , xxmaj finite xxmaj feedback xxmaj space - xxmaj time xxmaj schemes with xxmaj minimum xxmaj feedback and xxmaj transmission xxmaj duration xxmaj in this paper a xxup mimo quasi static block fading channel with finite n - ary delay - free , noise - free feedback is considered . xxmaj the transmitter uses a set of n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj structured xxmaj lattice xxmaj codes for xxmaj some xxmaj two - xxmaj user xxmaj gaussian xxmaj networks with xxmaj cognition , xxmaj coordination and xxmaj two xxmaj hops xxmaj we study a number of two - user interference networks with multiple - antenna transmitters / receivers , transmitter side information in the form of linear combinations ( over finite - field ) of the information messages , and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()  ## Glimpse of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (54731 items)\n",
       "x: TextList\n",
       "xxbos xxmaj nested satisfiability a special case of the satisfiability problem , in which the clauses have a hierarchical structure , is shown to be solvable in linear time , assuming that the clauses have been represented in a convenient way .,xxbos a note on digitized angles xxmaj we study the configurations of pixels that occur when two digitized straight lines meet each other .,xxbos xxmaj textbook examples of recursion xxmaj we discuss properties of recursive schemas related to mccarthy 's ` ` 91 function ' ' and to xxmaj takeuchi 's triple recursion . xxmaj several theorems are proposed as interesting candidates for machine verification , and some intriguing open questions are raised .,xxbos xxmaj theory and practice xxmaj the author argues to xxmaj silicon xxmaj valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical . xxmaj he particularly considers the intersection of the theory of algorithms and practical software development . xxmaj he combines examples from the development of the tex typesetting system with clever jokes , criticisms , and xxunk .,xxbos xxmaj context - free xxunk xxmaj this article is a sketch of ideas that were once intended to appear in the author 's famous series , xxmaj the xxmaj art of xxmaj computer xxmaj programming . xxmaj he generalizes the notion of a context - free language from a set to a multiset of words over an alphabet . xxmaj the idea is to keep track of the number of ways to parse a string . xxmaj for example , fruit flies like a xxunk can famously be parsed in two ways ; analogous examples in the setting of programming languages may yet be important in the future . xxmaj the treatment is informal but essentially rigorous .\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (54731 items)\n",
       "x: TextList\n",
       "xxbos xxmaj nested satisfiability a special case of the satisfiability problem , in which the clauses have a hierarchical structure , is shown to be solvable in linear time , assuming that the clauses have been represented in a convenient way .,xxbos a note on digitized angles xxmaj we study the configurations of pixels that occur when two digitized straight lines meet each other .,xxbos xxmaj textbook examples of recursion xxmaj we discuss properties of recursive schemas related to mccarthy 's ` ` 91 function ' ' and to xxmaj takeuchi 's triple recursion . xxmaj several theorems are proposed as interesting candidates for machine verification , and some intriguing open questions are raised .,xxbos xxmaj theory and practice xxmaj the author argues to xxmaj silicon xxmaj valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical . xxmaj he particularly considers the intersection of the theory of algorithms and practical software development . xxmaj he combines examples from the development of the tex typesetting system with clever jokes , criticisms , and xxunk .,xxbos xxmaj context - free xxunk xxmaj this article is a sketch of ideas that were once intended to appear in the author 's famous series , xxmaj the xxmaj art of xxmaj computer xxmaj programming . xxmaj he generalizes the notion of a context - free language from a set to a multiset of words over an alphabet . xxmaj the idea is to keep track of the number of ways to parse a string . xxmaj for example , fruit flies like a xxunk can famously be parsed in two ways ; analogous examples in the setting of programming languages may yet be important in the future . xxmaj the treatment is informal but essentially rigorous .\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (19678 items)\n",
       "x: TextList\n",
       "xxbos a xxmaj data xxmaj transparency xxmaj framework for xxmaj mobile xxmaj applications xxmaj in today 's mobile application marketplace , the ability of consumers to make informed choices regarding their privacy is extremely limited . xxmaj consumers largely rely on privacy policies and app permission mechanisms , but these do an inadequate job of conveying how information will be collected , used , stored , and shared . xxmaj mobile application developers go largely xxunk for making apps more privacy conscious as it is difficult to communicate these features to consumers while they are searching for a new app . xxmaj this paper provides an overview of a framework designed to help consumers make informed choices , and an incentive mechanism to encourage app developers to implement it . xxmaj this framework includes machine readable privacy policies encouraged by mobile app stores and enhanced by user software agents . xxmaj such a framework would provide the foundation required for more advanced forms of privacy management to develop .,xxbos a xxunk scheduling problem arising in coal stockyard management xxmaj we study a number of variants of an abstract scheduling problem inspired by the scheduling of xxunk in the stockyard of a coal export terminal . xxmaj we analyze the complexity of each of the variants , providing complexity proofs for some and polynomial algorithms for others . xxmaj for one , especially interesting variant , we also develop a constant factor approximation algorithm .,xxbos xxmaj communication - xxmaj efficient xxmaj distributed xxmaj optimization of xxmaj self - xxmaj concordant xxmaj empirical xxmaj loss xxmaj we consider distributed convex optimization problems originated from sample average approximation of stochastic optimization , or empirical risk minimization in machine learning . xxmaj we assume that each machine in the distributed computing system has access to a local empirical loss function , constructed with i.i.d . data sampled from a common distribution . xxmaj we propose a communication - efficient distributed algorithm to minimize the overall empirical loss , which is the average of the local empirical losses . xxmaj the algorithm is based on an inexact damped xxmaj newton method , where the inexact xxmaj newton steps are computed by a distributed preconditioned conjugate gradient method . xxmaj we analyze its iteration complexity and communication efficiency for minimizing self - concordant empirical loss functions , and discuss the results for distributed ridge regression , logistic regression and binary classification with a smoothed hinge loss . xxmaj in a standard setting for supervised learning , the required number of communication rounds of the algorithm does not increase with the sample size , and only grows slowly with the number of machines .,xxbos xxmaj consistent xxmaj classification xxmaj algorithms for xxmaj multi - class xxmaj non - xxmaj decomposable xxmaj performance xxmaj metrics xxmaj we study consistency of learning algorithms for a multi - class performance metric that is a non - decomposable function of the confusion matrix of a classifier and can not be expressed as a sum of losses on individual data points ; examples of such performance metrics include the macro f - measure popular in information retrieval and the g - mean metric used in class - imbalanced problems . xxmaj while there has been much work in recent years in understanding the consistency properties of learning algorithms for ` binary ' non - decomposable metrics , little is known either about the form of the optimal classifier for a general multi - class non - decomposable metric , or about how these learning algorithms generalize to the multi - class case . xxmaj in this paper , we provide a unified framework for analysing a multi - class non - decomposable performance metric , where the problem of finding the optimal classifier for the performance metric is viewed as an optimization problem over the space of all confusion matrices achievable under the given distribution . xxmaj using this framework , we show that ( under a continuous distribution ) the optimal classifier for a multi - class performance metric can be obtained as the solution of a cost - sensitive classification problem , thus generalizing several previous results on specific binary non - decomposable metrics . xxmaj we then design a consistent learning algorithm for concave multi - class performance metrics that proceeds via a sequence of cost - sensitive classification problems , and can be seen as applying the conditional gradient xxup cg optimization method over the space of feasible confusion matrices . xxmaj to our knowledge , this is the first efficient learning algorithm ( whose running time is polynomial in the number of classes ) that is consistent for a large family of multi - class non - decomposable metrics . xxmaj our consistency proof uses a novel technique based on the convergence analysis of the xxup cg method .,xxbos xxmaj managing key multicasting through orthogonal systems xxmaj in this paper we propose a new protocol to manage multicast key distribution . xxmaj the protocol is based on the use of orthogonal systems in vector spaces . xxmaj the main advantage in comparison to other existing multicast key management protocols is that the length and the number of the messages which have to be sent are considerably smaller . xxmaj this makes the protocol especially attractive when the number of legitimate receivers is large .\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ., model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f1f2ec165f0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here we load our encoder that we had saved and create the classifier with the architecture of AWD_LSTM which has been pre-defined with pre-trained model that is provided by fastai.\n",
    "## Regularization parameter is set as 0.5 which is usually the standard parameter.\n",
    "\n",
    "classifier = text_classifier_learner(data_clas, arch= AWD_LSTM,drop_mult=0.5)\n",
    "classifier.load_encoder('/kaggle/working/encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The architecture AWD_LSTM stands for ASGD Weight Dropped LSTM where ASGD stands for Asynchronous Stochastic Gradient Descent and LSTM stands for Long Short Term memory.` `The reason we are using LSTM because it is one of the best structures for processing and learning sequential sequences and text classification.As here we have used the neural network for the` `language model too where we had to predict the next work given a few words in same window size. The AWD_LSTM is the pre-trained model consists an input dimension of the vocabulary and an` `output dimension of 300. It consists of weight dropout, RNN dropout, input, encoder dropout, dropout is a way to prevent the neural network from overfitting it is used as a regularization method.` </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.303351</td>\n",
       "      <td>0.262195</td>\n",
       "      <td>0.887267</td>\n",
       "      <td>08:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier.fit_one_cycle(1, 1e-3)  ## Then we fit the classifier using fit_one_cycle for the last few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.317266</td>\n",
       "      <td>0.250214</td>\n",
       "      <td>0.894064</td>\n",
       "      <td>08:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.294563</td>\n",
       "      <td>0.230826</td>\n",
       "      <td>0.904003</td>\n",
       "      <td>08:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.277894</td>\n",
       "      <td>0.208725</td>\n",
       "      <td>0.912591</td>\n",
       "      <td>08:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.235446</td>\n",
       "      <td>0.202674</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>08:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Then unfreeze all the layers and apply the classifier to all the layers.\n",
    "\n",
    "\n",
    "classifier.unfreeze()\n",
    "classifier.fit_one_cycle(4,slice(1e-3/100,1e-3),moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will now chcek the accuracy on the test set and get the predictions using the get_preds function to get the prediction probabilities of teh abstract belonging to either class and the prediction labels. The parameters are the test dataset and whether the result should be ordered or not and then check what the probabilities are, as we can see for the first abstract the abstract has a probability of 0.99 of being 0 and 0.0018 of being 1 so it means it would belong to label 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9982, 0.0018],\n",
       "        [0.9102, 0.0898],\n",
       "        [0.4887, 0.5113],\n",
       "        ...,\n",
       "        [0.9912, 0.0088],\n",
       "        [0.9979, 0.0021],\n",
       "        [0.9798, 0.0202]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, target = classifier.get_preds(ds_type = DatasetType.Test, ordered=True) ## Used the inbuilt fastai function called get_preds which returns predictions and target class.\n",
    "print('The preds as tensors is:',preds) \n",
    "preds = to_np(preds)  ## convert to an array for calculating the confusion matrix.\n",
    "print('The preds after being converted to a numpy array',preds)  ## glimpse of the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19678,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=test_set['Math'].dropna(how='all')\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function to get the predicted labels for the abstracts in the test dataset. If the probability of it lying in class '0' is higher than it lying in class '1' then the abstract would\n",
    "## lie in class 0 and vice-versa.\n",
    "\n",
    "\n",
    "def get_label(preds):\n",
    "    if preds[0] > preds[1]:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a list for all the predicted labels in order to calculate the confusion matrix so append all the labels calculated using the function get_label to y_predict/\n",
    "\n",
    "\n",
    "y_predict = list()    \n",
    "\n",
    "for prediction in preds:\n",
    "    y_predict.append(get_label(prediction))\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported the libraries sklearn.metrics to calculate the metrics for a classification model such as the true positives and negatives and false negatives and positives, the precision(which is basically the count of positives that were actual posiitves from the predicted positives and the recall is the sensitivity which checks how many actual posiitves our model can capture and F1 score is )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Precision = \\frac{True Postive}{True Positive + True negative}$$<br>\n",
    "$$= \\frac{True Positive}{Total Predicted Positive}$$ <br>\n",
    "\n",
    "`Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.It is a good measure to determine when the costs of False Positive are high that means values that are actually positive but classified as negative.`\n",
    "\n",
    "$$Recall = \\frac{True Positive}{True Positive + False Positive}$$ <br>\n",
    "$$ = \\frac{True Positive}{Total Actual Positive}$$ <br>\n",
    "\n",
    "\n",
    "`Recall actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive).It is a good metric when the cost is high associated with False Negative.`\n",
    "\n",
    "$$F1\\ Score  = 2 \\times \\frac{precision \\times recall}{precision + recall}$$\n",
    "<br>\n",
    "`Is a good metric when we want an even class distribution or a way to seek good balance of true negatives and true positives as the accuracy is mostly attributed by classifying true negatives more than true positives.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Confusion Matrix:',\n",
       " array([[12749,   999],\n",
       "        [ 1241,  4689]]),\n",
       " 'Accuracy: 0.8861672934241285',\n",
       " 'Macro Precision: 0.8678304349399661',\n",
       " 'Macro Recall: 0.8590300057748703',\n",
       " 'Macro F1 score:0.863220044341576',\n",
       " 'MCC:0.7268071633677508')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "def confusionmatrix(y_predict,y_test):\n",
    "    \n",
    "\n",
    "    y_predict = np.asarray(y_predict)\n",
    "    y_test = np.asarray(y_test)\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    return('Confusion Matrix:', confusion_matrix(y_test,y_predict),'Accuracy: '+ str(accuracy),'Macro Precision: '+ str(precision),'Macro Recall: '+ str(recall),\\\n",
    "    'Macro F1 score:'+ str(f1score),'MCC:'+ str(matthews))\n",
    "\n",
    "confusionmatrix(y_predict,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier - 2(CompVis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompVis</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nested satisfiability A special case of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A note on digitized angles We study the confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Textbook examples of recursion We discuss pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Theory and practice The author argues to Sili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Context-free multilanguages This article is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompVis                                           Abstract\n",
       "0        0   Nested satisfiability A special case of the s...\n",
       "1        0   A note on digitized angles We study the confi...\n",
       "2        0   Textbook examples of recursion We discuss pro...\n",
       "3        0   Theory and practice The author argues to Sili...\n",
       "4        0   Context-free multilanguages This article is a..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comp= train_set[['CompVis','Abstract']]\n",
    "train_comp.head()  ## Glimpse of the modified sliced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19678, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comp = test_set[['CompVis', 'Abstract']]  \n",
    "test_comp= test_comp.dropna(how='all')\n",
    "test_comp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas_comp = TextClasDataBunch.from_df('./', train_df=train_comp, valid_df=train_comp, test_df= test_comp, vocab=data_bunch.train_ds.vocab, bs=32,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj some xxmaj extensions of xxmaj probabilistic xxmaj logic xxmaj in [ 12 ] , xxmaj nilsson proposed the probabilistic logic in which the truth values of logical propositions are probability values between 0 and 1 . xxmaj it is applicable to any logical system for which the consistency of a finite set of propositions can be established . xxmaj the probabilistic inference scheme reduces to the ordinary logical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj packing 3-vertex paths in claw - free graphs and related topics xxmaj an l - factor of a graph g is a spanning subgraph of g whose every component is a 3-vertex path . xxmaj let v(g ) be the number of vertices of g and d(g ) the domination number of xxup g. a claw is a graph with four vertices and three edges incident to the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj subspace xxmaj alignment xxmaj chains and the xxmaj degrees of xxmaj freedom of the xxmaj three - xxmaj user xxup mimo xxmaj interference xxmaj channel xxmaj we show that the 3 user xxup m_t x xxup m_r xxup mimo interference channel has d(m , xxunk / ( 2 - 1 / xxunk / ( 2 + 1 / k ) ) degrees of freedom ( dof ) normalized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj identification de r \\ ^oles xxunk dans des r \\ ' eseaux orient \\ ' es appliqu \\ ' ee \\ ` a xxmaj twitter xxmaj the notion of community structure is particularly useful when analyzing complex networks , because it provides an intermediate level , compared to the more classic global ( whole network ) and local ( node neighborhood ) approaches . xxmaj the concept of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj approximation xxmaj algorithms for xxmaj orienteering with xxmaj time xxmaj windows xxmaj orienteering is the following optimization problem : given an edge - weighted graph ( directed or undirected ) , two nodes s , t and a time limit t , find an s - t walk of total length at most t that maximizes the number of distinct nodes visited by the walk . xxmaj one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas_comp.show_batch()  ## Glimpse of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (54731 items)\n",
       "x: TextList\n",
       "xxbos xxmaj nested satisfiability a special case of the satisfiability problem , in which the clauses have a hierarchical structure , is shown to be solvable in linear time , assuming that the clauses have been represented in a convenient way .,xxbos a note on digitized angles xxmaj we study the configurations of pixels that occur when two digitized straight lines meet each other .,xxbos xxmaj textbook examples of recursion xxmaj we discuss properties of recursive schemas related to mccarthy 's ` ` 91 function ' ' and to xxmaj takeuchi 's triple recursion . xxmaj several theorems are proposed as interesting candidates for machine verification , and some intriguing open questions are raised .,xxbos xxmaj theory and practice xxmaj the author argues to xxmaj silicon xxmaj valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical . xxmaj he particularly considers the intersection of the theory of algorithms and practical software development . xxmaj he combines examples from the development of the tex typesetting system with clever jokes , criticisms , and xxunk .,xxbos xxmaj context - free xxunk xxmaj this article is a sketch of ideas that were once intended to appear in the author 's famous series , xxmaj the xxmaj art of xxmaj computer xxmaj programming . xxmaj he generalizes the notion of a context - free language from a set to a multiset of words over an alphabet . xxmaj the idea is to keep track of the number of ways to parse a string . xxmaj for example , fruit flies like a xxunk can famously be parsed in two ways ; analogous examples in the setting of programming languages may yet be important in the future . xxmaj the treatment is informal but essentially rigorous .\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (54731 items)\n",
       "x: TextList\n",
       "xxbos xxmaj nested satisfiability a special case of the satisfiability problem , in which the clauses have a hierarchical structure , is shown to be solvable in linear time , assuming that the clauses have been represented in a convenient way .,xxbos a note on digitized angles xxmaj we study the configurations of pixels that occur when two digitized straight lines meet each other .,xxbos xxmaj textbook examples of recursion xxmaj we discuss properties of recursive schemas related to mccarthy 's ` ` 91 function ' ' and to xxmaj takeuchi 's triple recursion . xxmaj several theorems are proposed as interesting candidates for machine verification , and some intriguing open questions are raised .,xxbos xxmaj theory and practice xxmaj the author argues to xxmaj silicon xxmaj valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical . xxmaj he particularly considers the intersection of the theory of algorithms and practical software development . xxmaj he combines examples from the development of the tex typesetting system with clever jokes , criticisms , and xxunk .,xxbos xxmaj context - free xxunk xxmaj this article is a sketch of ideas that were once intended to appear in the author 's famous series , xxmaj the xxmaj art of xxmaj computer xxmaj programming . xxmaj he generalizes the notion of a context - free language from a set to a multiset of words over an alphabet . xxmaj the idea is to keep track of the number of ways to parse a string . xxmaj for example , fruit flies like a xxunk can famously be parsed in two ways ; analogous examples in the setting of programming languages may yet be important in the future . xxmaj the treatment is informal but essentially rigorous .\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (19678 items)\n",
       "x: TextList\n",
       "xxbos a xxmaj data xxmaj transparency xxmaj framework for xxmaj mobile xxmaj applications xxmaj in today 's mobile application marketplace , the ability of consumers to make informed choices regarding their privacy is extremely limited . xxmaj consumers largely rely on privacy policies and app permission mechanisms , but these do an inadequate job of conveying how information will be collected , used , stored , and shared . xxmaj mobile application developers go largely xxunk for making apps more privacy conscious as it is difficult to communicate these features to consumers while they are searching for a new app . xxmaj this paper provides an overview of a framework designed to help consumers make informed choices , and an incentive mechanism to encourage app developers to implement it . xxmaj this framework includes machine readable privacy policies encouraged by mobile app stores and enhanced by user software agents . xxmaj such a framework would provide the foundation required for more advanced forms of privacy management to develop .,xxbos a xxunk scheduling problem arising in coal stockyard management xxmaj we study a number of variants of an abstract scheduling problem inspired by the scheduling of xxunk in the stockyard of a coal export terminal . xxmaj we analyze the complexity of each of the variants , providing complexity proofs for some and polynomial algorithms for others . xxmaj for one , especially interesting variant , we also develop a constant factor approximation algorithm .,xxbos xxmaj communication - xxmaj efficient xxmaj distributed xxmaj optimization of xxmaj self - xxmaj concordant xxmaj empirical xxmaj loss xxmaj we consider distributed convex optimization problems originated from sample average approximation of stochastic optimization , or empirical risk minimization in machine learning . xxmaj we assume that each machine in the distributed computing system has access to a local empirical loss function , constructed with i.i.d . data sampled from a common distribution . xxmaj we propose a communication - efficient distributed algorithm to minimize the overall empirical loss , which is the average of the local empirical losses . xxmaj the algorithm is based on an inexact damped xxmaj newton method , where the inexact xxmaj newton steps are computed by a distributed preconditioned conjugate gradient method . xxmaj we analyze its iteration complexity and communication efficiency for minimizing self - concordant empirical loss functions , and discuss the results for distributed ridge regression , logistic regression and binary classification with a smoothed hinge loss . xxmaj in a standard setting for supervised learning , the required number of communication rounds of the algorithm does not increase with the sample size , and only grows slowly with the number of machines .,xxbos xxmaj consistent xxmaj classification xxmaj algorithms for xxmaj multi - class xxmaj non - xxmaj decomposable xxmaj performance xxmaj metrics xxmaj we study consistency of learning algorithms for a multi - class performance metric that is a non - decomposable function of the confusion matrix of a classifier and can not be expressed as a sum of losses on individual data points ; examples of such performance metrics include the macro f - measure popular in information retrieval and the g - mean metric used in class - imbalanced problems . xxmaj while there has been much work in recent years in understanding the consistency properties of learning algorithms for ` binary ' non - decomposable metrics , little is known either about the form of the optimal classifier for a general multi - class non - decomposable metric , or about how these learning algorithms generalize to the multi - class case . xxmaj in this paper , we provide a unified framework for analysing a multi - class non - decomposable performance metric , where the problem of finding the optimal classifier for the performance metric is viewed as an optimization problem over the space of all confusion matrices achievable under the given distribution . xxmaj using this framework , we show that ( under a continuous distribution ) the optimal classifier for a multi - class performance metric can be obtained as the solution of a cost - sensitive classification problem , thus generalizing several previous results on specific binary non - decomposable metrics . xxmaj we then design a consistent learning algorithm for concave multi - class performance metrics that proceeds via a sequence of cost - sensitive classification problems , and can be seen as applying the conditional gradient xxup cg optimization method over the space of feasible confusion matrices . xxmaj to our knowledge , this is the first efficient learning algorithm ( whose running time is polynomial in the number of classes ) that is consistent for a large family of multi - class non - decomposable metrics . xxmaj our consistency proof uses a novel technique based on the convergence analysis of the xxup cg method .,xxbos xxmaj managing key multicasting through orthogonal systems xxmaj in this paper we propose a new protocol to manage multicast key distribution . xxmaj the protocol is based on the use of orthogonal systems in vector spaces . xxmaj the main advantage in comparison to other existing multicast key management protocols is that the length and the number of the messages which have to be sent are considerably smaller . xxmaj this makes the protocol especially attractive when the number of legitimate receivers is large .\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ., model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f1f2ec165f0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_comp = text_classifier_learner(data_clas_comp, arch= AWD_LSTM,drop_mult=0.5)\n",
    "classifier_comp.load_encoder('/kaggle/working/encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.069779</td>\n",
       "      <td>0.041521</td>\n",
       "      <td>0.985438</td>\n",
       "      <td>08:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_comp.fit_one_cycle(1, 1e-3)  ## Then we fit the classifier using fit_one_cycle for the last few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.054240</td>\n",
       "      <td>0.041381</td>\n",
       "      <td>0.986023</td>\n",
       "      <td>08:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.028016</td>\n",
       "      <td>0.989659</td>\n",
       "      <td>08:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>0.992472</td>\n",
       "      <td>08:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.027129</td>\n",
       "      <td>0.019136</td>\n",
       "      <td>0.993185</td>\n",
       "      <td>08:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_comp.unfreeze()\n",
    "classifier_comp.fit_one_cycle(4,slice(1e-3/100,1e-3),moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[9.999949e-01, 5.146842e-06],\n",
       "       [9.999942e-01, 5.831003e-06],\n",
       "       [9.999985e-01, 1.579454e-06],\n",
       "       [9.938254e-01, 6.174563e-03],\n",
       "       ...,\n",
       "       [9.932635e-01, 6.736453e-03],\n",
       "       [9.999989e-01, 1.114235e-06],\n",
       "       [9.978639e-01, 2.136070e-03],\n",
       "       [1.547193e-01, 8.452807e-01]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_comp, target_comp = classifier_comp.get_preds(ds_type = DatasetType.Test, ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_comp=list()\n",
    "\n",
    "for comp in preds_comp:\n",
    "    y_predict_comp.append(get_label(comp))\n",
    "y_predict_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19678,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_comp= test_set['CompVis'].dropna(how='all')\n",
    "y_test_comp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Confusion Matrix:',\n",
       " array([[17396,   130],\n",
       "        [  312,  1840]]),\n",
       " 'Accuracy: 0.9775383677202968',\n",
       " 'Macro Precision: 0.9581954985500831',\n",
       " 'Macro Recall: 0.9238005181468043',\n",
       " 'Macro F1 score:0.940112899308228',\n",
       " 'MCC:0.8813251152623726')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionmatrix(y_predict_comp,y_test_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier - 3 (InfoTheory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InfoTheory</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nested satisfiability A special case of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A note on digitized angles We study the confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Textbook examples of recursion We discuss pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Theory and practice The author argues to Sili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Context-free multilanguages This article is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   InfoTheory                                           Abstract\n",
       "0           0   Nested satisfiability A special case of the s...\n",
       "1           0   A note on digitized angles We study the confi...\n",
       "2           0   Textbook examples of recursion We discuss pro...\n",
       "3           0   Theory and practice The author argues to Sili...\n",
       "4           0   Context-free multilanguages This article is a..."
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_info= train_set[['InfoTheory','Abstract']]\n",
    "train_info.head()  ## Glimpse of the modified sliced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19678, 2)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_info = test_set[['InfoTheory', 'Abstract']]  \n",
    "test_info= test_info.dropna(how='all')\n",
    "test_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas_info = TextClasDataBunch.from_df('./', train_df=train_info, valid_df=train_info, test_df= test_info, vocab=data_bunch.train_ds.vocab, bs=32,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj some xxmaj extensions of xxmaj probabilistic xxmaj logic xxmaj in [ 12 ] , xxmaj nilsson proposed the probabilistic logic in which the truth values of logical propositions are probability values between 0 and 1 . xxmaj it is applicable to any logical system for which the consistency of a finite set of propositions can be established . xxmaj the probabilistic inference scheme reduces to the ordinary logical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj packing 3-vertex paths in claw - free graphs and related topics xxmaj an l - factor of a graph g is a spanning subgraph of g whose every component is a 3-vertex path . xxmaj let v(g ) be the number of vertices of g and d(g ) the domination number of xxup g. a claw is a graph with four vertices and three edges incident to the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj subspace xxmaj alignment xxmaj chains and the xxmaj degrees of xxmaj freedom of the xxmaj three - xxmaj user xxup mimo xxmaj interference xxmaj channel xxmaj we show that the 3 user xxup m_t x xxup m_r xxup mimo interference channel has d(m , xxunk / ( 2 - 1 / xxunk / ( 2 + 1 / k ) ) degrees of freedom ( dof ) normalized</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj identification de r \\ ^oles xxunk dans des r \\ ' eseaux orient \\ ' es appliqu \\ ' ee \\ ` a xxmaj twitter xxmaj the notion of community structure is particularly useful when analyzing complex networks , because it provides an intermediate level , compared to the more classic global ( whole network ) and local ( node neighborhood ) approaches . xxmaj the concept of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj approximation xxmaj algorithms for xxmaj orienteering with xxmaj time xxmaj windows xxmaj orienteering is the following optimization problem : given an edge - weighted graph ( directed or undirected ) , two nodes s , t and a time limit t , find an s - t walk of total length at most t that maximizes the number of distinct nodes visited by the walk . xxmaj one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas_info.show_batch()  ## Glimpse of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (54731 items)\n",
       "x: TextList\n",
       "xxbos xxmaj nested satisfiability a special case of the satisfiability problem , in which the clauses have a hierarchical structure , is shown to be solvable in linear time , assuming that the clauses have been represented in a convenient way .,xxbos a note on digitized angles xxmaj we study the configurations of pixels that occur when two digitized straight lines meet each other .,xxbos xxmaj textbook examples of recursion xxmaj we discuss properties of recursive schemas related to mccarthy 's ` ` 91 function ' ' and to xxmaj takeuchi 's triple recursion . xxmaj several theorems are proposed as interesting candidates for machine verification , and some intriguing open questions are raised .,xxbos xxmaj theory and practice xxmaj the author argues to xxmaj silicon xxmaj valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical . xxmaj he particularly considers the intersection of the theory of algorithms and practical software development . xxmaj he combines examples from the development of the tex typesetting system with clever jokes , criticisms , and xxunk .,xxbos xxmaj context - free xxunk xxmaj this article is a sketch of ideas that were once intended to appear in the author 's famous series , xxmaj the xxmaj art of xxmaj computer xxmaj programming . xxmaj he generalizes the notion of a context - free language from a set to a multiset of words over an alphabet . xxmaj the idea is to keep track of the number of ways to parse a string . xxmaj for example , fruit flies like a xxunk can famously be parsed in two ways ; analogous examples in the setting of programming languages may yet be important in the future . xxmaj the treatment is informal but essentially rigorous .\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (54731 items)\n",
       "x: TextList\n",
       "xxbos xxmaj nested satisfiability a special case of the satisfiability problem , in which the clauses have a hierarchical structure , is shown to be solvable in linear time , assuming that the clauses have been represented in a convenient way .,xxbos a note on digitized angles xxmaj we study the configurations of pixels that occur when two digitized straight lines meet each other .,xxbos xxmaj textbook examples of recursion xxmaj we discuss properties of recursive schemas related to mccarthy 's ` ` 91 function ' ' and to xxmaj takeuchi 's triple recursion . xxmaj several theorems are proposed as interesting candidates for machine verification , and some intriguing open questions are raised .,xxbos xxmaj theory and practice xxmaj the author argues to xxmaj silicon xxmaj valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical . xxmaj he particularly considers the intersection of the theory of algorithms and practical software development . xxmaj he combines examples from the development of the tex typesetting system with clever jokes , criticisms , and xxunk .,xxbos xxmaj context - free xxunk xxmaj this article is a sketch of ideas that were once intended to appear in the author 's famous series , xxmaj the xxmaj art of xxmaj computer xxmaj programming . xxmaj he generalizes the notion of a context - free language from a set to a multiset of words over an alphabet . xxmaj the idea is to keep track of the number of ways to parse a string . xxmaj for example , fruit flies like a xxunk can famously be parsed in two ways ; analogous examples in the setting of programming languages may yet be important in the future . xxmaj the treatment is informal but essentially rigorous .\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (19678 items)\n",
       "x: TextList\n",
       "xxbos a xxmaj data xxmaj transparency xxmaj framework for xxmaj mobile xxmaj applications xxmaj in today 's mobile application marketplace , the ability of consumers to make informed choices regarding their privacy is extremely limited . xxmaj consumers largely rely on privacy policies and app permission mechanisms , but these do an inadequate job of conveying how information will be collected , used , stored , and shared . xxmaj mobile application developers go largely xxunk for making apps more privacy conscious as it is difficult to communicate these features to consumers while they are searching for a new app . xxmaj this paper provides an overview of a framework designed to help consumers make informed choices , and an incentive mechanism to encourage app developers to implement it . xxmaj this framework includes machine readable privacy policies encouraged by mobile app stores and enhanced by user software agents . xxmaj such a framework would provide the foundation required for more advanced forms of privacy management to develop .,xxbos a xxunk scheduling problem arising in coal stockyard management xxmaj we study a number of variants of an abstract scheduling problem inspired by the scheduling of xxunk in the stockyard of a coal export terminal . xxmaj we analyze the complexity of each of the variants , providing complexity proofs for some and polynomial algorithms for others . xxmaj for one , especially interesting variant , we also develop a constant factor approximation algorithm .,xxbos xxmaj communication - xxmaj efficient xxmaj distributed xxmaj optimization of xxmaj self - xxmaj concordant xxmaj empirical xxmaj loss xxmaj we consider distributed convex optimization problems originated from sample average approximation of stochastic optimization , or empirical risk minimization in machine learning . xxmaj we assume that each machine in the distributed computing system has access to a local empirical loss function , constructed with i.i.d . data sampled from a common distribution . xxmaj we propose a communication - efficient distributed algorithm to minimize the overall empirical loss , which is the average of the local empirical losses . xxmaj the algorithm is based on an inexact damped xxmaj newton method , where the inexact xxmaj newton steps are computed by a distributed preconditioned conjugate gradient method . xxmaj we analyze its iteration complexity and communication efficiency for minimizing self - concordant empirical loss functions , and discuss the results for distributed ridge regression , logistic regression and binary classification with a smoothed hinge loss . xxmaj in a standard setting for supervised learning , the required number of communication rounds of the algorithm does not increase with the sample size , and only grows slowly with the number of machines .,xxbos xxmaj consistent xxmaj classification xxmaj algorithms for xxmaj multi - class xxmaj non - xxmaj decomposable xxmaj performance xxmaj metrics xxmaj we study consistency of learning algorithms for a multi - class performance metric that is a non - decomposable function of the confusion matrix of a classifier and can not be expressed as a sum of losses on individual data points ; examples of such performance metrics include the macro f - measure popular in information retrieval and the g - mean metric used in class - imbalanced problems . xxmaj while there has been much work in recent years in understanding the consistency properties of learning algorithms for ` binary ' non - decomposable metrics , little is known either about the form of the optimal classifier for a general multi - class non - decomposable metric , or about how these learning algorithms generalize to the multi - class case . xxmaj in this paper , we provide a unified framework for analysing a multi - class non - decomposable performance metric , where the problem of finding the optimal classifier for the performance metric is viewed as an optimization problem over the space of all confusion matrices achievable under the given distribution . xxmaj using this framework , we show that ( under a continuous distribution ) the optimal classifier for a multi - class performance metric can be obtained as the solution of a cost - sensitive classification problem , thus generalizing several previous results on specific binary non - decomposable metrics . xxmaj we then design a consistent learning algorithm for concave multi - class performance metrics that proceeds via a sequence of cost - sensitive classification problems , and can be seen as applying the conditional gradient xxup cg optimization method over the space of feasible confusion matrices . xxmaj to our knowledge , this is the first efficient learning algorithm ( whose running time is polynomial in the number of classes ) that is consistent for a large family of multi - class non - decomposable metrics . xxmaj our consistency proof uses a novel technique based on the convergence analysis of the xxup cg method .,xxbos xxmaj managing key multicasting through orthogonal systems xxmaj in this paper we propose a new protocol to manage multicast key distribution . xxmaj the protocol is based on the use of orthogonal systems in vector spaces . xxmaj the main advantage in comparison to other existing multicast key management protocols is that the length and the number of the messages which have to be sent are considerably smaller . xxmaj this makes the protocol especially attractive when the number of legitimate receivers is large .\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ., model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f1f2ec165f0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_info = text_classifier_learner(data_clas_info, arch= AWD_LSTM,drop_mult=0.5)\n",
    "classifier_info.load_encoder('/kaggle/working/encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.151481</td>\n",
       "      <td>0.117127</td>\n",
       "      <td>0.957465</td>\n",
       "      <td>08:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_info.fit_one_cycle(1, 1e-3)  ## Then we fit the classifier using fit_one_cycle for the last few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.160053</td>\n",
       "      <td>0.109645</td>\n",
       "      <td>0.960205</td>\n",
       "      <td>08:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.134429</td>\n",
       "      <td>0.092023</td>\n",
       "      <td>0.966929</td>\n",
       "      <td>08:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109189</td>\n",
       "      <td>0.077602</td>\n",
       "      <td>0.971716</td>\n",
       "      <td>08:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101736</td>\n",
       "      <td>0.073992</td>\n",
       "      <td>0.972849</td>\n",
       "      <td>08:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_info.unfreeze()\n",
    "classifier_info.fit_one_cycle(4,slice(1e-3/100,1e-3),moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_info, target_info = classifier_info.get_preds(ds_type = DatasetType.Test, ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9982, 0.0018],\n",
       "        [0.9102, 0.0898],\n",
       "        [0.4887, 0.5113],\n",
       "        ...,\n",
       "        [0.9912, 0.0088],\n",
       "        [0.9979, 0.0021],\n",
       "        [0.9798, 0.0202]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_info=list()\n",
    "\n",
    "for info in preds_info:\n",
    "    predict_info.append(get_label(info))\n",
    "predict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19678,)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_info= test_set['InfoTheory'].dropna(how='all')\n",
    "y_test_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Confusion Matrix:',\n",
       " array([[15767,   295],\n",
       "        [  472,  3144]]),\n",
       " 'Accuracy: 0.9610224616322797',\n",
       " 'Macro Precision: 0.9425767103026199',\n",
       " 'Macro Recall: 0.9255513480396208',\n",
       " 'Macro F1 score:0.933768691646047',\n",
       " 'MCC:0.8679610951654934')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionmatrix(predict_info,y_test_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
